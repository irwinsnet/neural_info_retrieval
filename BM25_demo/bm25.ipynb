{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bdec0b-c2bf-4058-b6e5-2247327bf56e",
   "metadata": {},
   "source": [
    "# BM25 Algorithm\n",
    "BM25 is a probabilistic algorithm for information retrieval that is described in [1]. It is summarized here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683f236-a978-4e31-b80c-b667a7fdee0f",
   "metadata": {},
   "source": [
    "## 1. Msmarco Dataset\n",
    "We'll use the [msmarco](https://microsoft.github.io/msmarco/TREC-Deep-Learning-2019) dataset for examples in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8281ff5-c781-4eb9-88ab-9fe8c4c3bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1555982\thttps://answers.yahoo.com/question/index?qid=20071007114826AAwCFvR\tThe hot glowing surfaces of stars emit energy in the form of electromagnetic radiation.?\tScience & Mathematics Physics The hot glowing surfaces of stars emit energy in the form of electromagnetic radiation.? It is a good approximation to assume that the emissivity e is equal to 1 for these surfaces. Find the radius of the star Rigel, the bright blue star in the constellation Orion that radiates energy at a rate of 2.7 x 10^32 W and has a surface temperature of 11,000 K. Assume that the star is spherical. Use σ =... show more Follow 3 answers Answers Relevance Rating Newest Oldest Best Answer: Stefan-Boltzmann law states that the energy flux by radiation is proportional to the forth power of the temperature: q = ε · σ · T^4 The total energy flux at a spherical surface of Radius R is Q = q·π·R² = ε·σ·T^4·π·R² Hence the radius is R = √ ( Q / (ε·σ·T^4·π) ) = √ ( 2.7x10+32 W / (1 · 5.67x10-8W/m²K^4 · (1100K)^4 · π) ) = 3.22x10+13 m Source (s):http://en.wikipedia.org/wiki/Stefan_bolt...schmiso · 1 decade ago0 18 Comment Schmiso, you forgot a 4 in your answer. Your link even says it: L = 4pi (R^2)sigma (T^4). Using L, luminosity, as the energy in this problem, you can find the radius R by doing sqrt (L/ (4pisigma (T^4)). Hope this helps everyone. Caroline · 4 years ago4 1 Comment (Stefan-Boltzmann law) L = 4pi*R^2*sigma*T^4 Solving for R we get: => R = (1/ (2T^2)) * sqrt (L/ (pi*sigma)) Plugging in your values you should get: => R = (1/ (2 (11,000K)^2)) *sqrt ( (2.7*10^32W)/ (pi * (5.67*10^-8 W/m^2K^4))) R = 1.609 * 10^11 m? · 3 years ago0 1 Comment Maybe you would like to learn more about one of these? Want to build a free website? Interested in dating sites? Need a Home Security Safe? How to order contacts online? \n"
     ]
    }
   ],
   "source": [
    "# msmarco-docs.tsv not uploaded to github due to size.\n",
    "# !head ../../data/msmarco/msmarco-docs.tsv -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0784b53-8dd7-472c-9bd2-ec882d9c42ff",
   "metadata": {},
   "source": [
    "Each line of the `msmarco-docs.tsv` contains one document, with four fields per document: document ID, URL, title, and body.\n",
    "\n",
    "The `msmarco-docs.tsv` file contains 3.2 million documents in one 22GB file, which is cumbersome to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abdfcdd-0a12-4f0a-98bd-d40fa3cccde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3213835 ../data/msmarco/msmarco-docs.tsv\n"
     ]
    }
   ],
   "source": [
    "# msmarco-docs.tsv not uploaded to github due to size.\n",
    "# !wc ../../data/msmarco/docs/msmarco-docs.tsv -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1814f7-0081-4990-a0f4-9e5eeae63035",
   "metadata": {},
   "source": [
    "The Linux *split* command will split the TSV file into 64 documents, each containing 50,000 documents and several hundred MB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0bacde-8aae-43e4-95b7-de28e0527222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 22G\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  24K May 25 00:18 bm25.ipynb\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4.0K May 24 04:12 wordpiece\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 339M May 24 03:46 x00\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 347M May 24 03:46 x01\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 337M May 24 03:46 x02\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 342M May 24 03:46 x03\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 335M May 24 03:46 x04\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 340M May 24 03:46 x05\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 345M May 24 03:46 x06\n"
     ]
    }
   ],
   "source": [
    "#! split -d -l 50000 ../data/msmarco/msmarco-docs.tsv\n",
    "# ! ls -l -h | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1381770-c681-4bdf-be2f-7dcf62a1587d",
   "metadata": {},
   "source": [
    "## 2. Simple IDF-ish Algorithm\n",
    "### A. Math\n",
    "Eq (1)\n",
    "$$P(\\textrm{rel}|d, q) \\propto_q \\frac{P(\\textrm{rel}|d, q)}{P(\\overline{\\textrm{rel}}|d, q)}$$\n",
    "\n",
    "$P(\\textrm{rel}|d, q)$ is the probability that document $d$ is relevant for query $q$. Eq(1) shows that the probability is proportional to the odds ratio, which is the probability that the document is relevant over the probability that the document is not relevant.\n",
    "\n",
    "Next, a lot of math and approximations happen and we end up with Eq (2). See section 2.4 of [1] for details.\n",
    "\n",
    "Eq(2)\n",
    "$$P(\\textrm{rel}|d, q) \\propto_q \\sum_{\\textbf{q}, tf_i > 0} w_i$$\n",
    "\n",
    "$w_i$ is the weight for term $t_i$, which represents the importance of term $i$ in document $d$. We calculate the score of a document by summing the weights of all terms that appear in both the query $q$ and the document. We can then return the top $m$ documents in descending order of $P(\\textrm{rel}|d, q)$.\n",
    "\n",
    "A simple way of determining $w_i$ is to use Eq(3), which corresponds to equation 3.3 in [1]. This approach is very similar to a classical *idf* approach.\n",
    "\n",
    "Eq(3)\n",
    "$$w_i^{\\textrm{IDF}} = \\log\\frac{N - n_i + 0.5}{n_i + 0.5}$$\n",
    "\n",
    "N is the number of documents in the corpus and $n_i$ is the number of documents in the corpus that contain term $t_i$. For terms that appear in most documents, $n_i$ will be close to $N$ and $w_i^{\\textrm{IDF}}$ will be close to zero. The values of $0.5$ in the numerator and denominator smooth the weights in the event of rare query terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4242b92-0145-4721-9e2d-f6d15ae055d8",
   "metadata": {},
   "source": [
    "There are about 3.2 million documents in the corpus. Working with a single 22GB file is problematic. Splitting it into files with 50,000 docs each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5f073-6341-45ed-811b-b193132f5a31",
   "metadata": {},
   "source": [
    "### B. Information Retrieval Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d305bf-a193-436c-9939-8861b2edf34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a076cc3-ae09-47e6-82ee-e43ee385fd1d",
   "metadata": {},
   "source": [
    "We will index and create a retrieval system for a corpus of 1000 documents selected from an arbitrary text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f1ff95a-1820-4489-b7ef-d900855cec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D2273143</td>\n",
       "      <td>https://www.allaboutcircuits.com/textbook/refe...</td>\n",
       "      <td>Solving Simultaneous Equations</td>\n",
       "      <td>Solving Simultaneous Equations Chapter 4 - Alg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1029983</td>\n",
       "      <td>http://xynyth.com/resource/icemelter-concrete/...</td>\n",
       "      <td>.</td>\n",
       "      <td>Icemelters and Concrete- The basics, what ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D312314</td>\n",
       "      <td>http://medical-dictionary.thefreedictionary.co...</td>\n",
       "      <td>nurse's aide</td>\n",
       "      <td>nurse's aide Also found in: Dictionary, Thesau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D582292</td>\n",
       "      <td>https://www.business-case-analysis.com/cost-al...</td>\n",
       "      <td>Cost Allocation and Cost Apportionment Definit...</td>\n",
       "      <td>Home &gt; Encyclopedia &gt; C &gt; Cost Allocation Cost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1021364</td>\n",
       "      <td>http://mexicanspanish.com/equis/</td>\n",
       "      <td>Equis</td>\n",
       "      <td>Wednesday, January 27, 2016 by Mark Robert Ale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>D2376648</td>\n",
       "      <td>http://surnames.meaning-of-names.com/shirazi/</td>\n",
       "      <td>.</td>\n",
       "      <td>Shirazi Meaning &amp; Surname Resources Etymology ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>D2404743</td>\n",
       "      <td>http://www.answers.com/Q/What_is_the_Military_...</td>\n",
       "      <td>How do you edit and cut off email draft?</td>\n",
       "      <td>Min0619 2 Contributions How do you edit and cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>D3239066</td>\n",
       "      <td>http://www.rhythm-in-music.com/introduction-wh...</td>\n",
       "      <td>What Is Rhythm?</td>\n",
       "      <td>Learn about Rhythm Home - Fundamentals Sound W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>D3369056</td>\n",
       "      <td>http://www.dictionaryofengineering.com/definit...</td>\n",
       "      <td>.</td>\n",
       "      <td>Nearby Termsvirtual devicevirtual device drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>D2381163</td>\n",
       "      <td>https://en.wikipedia.org/wiki/United_States_mi...</td>\n",
       "      <td>United States military ration</td>\n",
       "      <td>From Wikipedia, the free encyclopedianavigatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_id                                                url  \\\n",
       "0     D2273143  https://www.allaboutcircuits.com/textbook/refe...   \n",
       "1     D1029983  http://xynyth.com/resource/icemelter-concrete/...   \n",
       "2      D312314  http://medical-dictionary.thefreedictionary.co...   \n",
       "3      D582292  https://www.business-case-analysis.com/cost-al...   \n",
       "4     D1021364                   http://mexicanspanish.com/equis/   \n",
       "...        ...                                                ...   \n",
       "4995  D2376648      http://surnames.meaning-of-names.com/shirazi/   \n",
       "4996  D2404743  http://www.answers.com/Q/What_is_the_Military_...   \n",
       "4997  D3239066  http://www.rhythm-in-music.com/introduction-wh...   \n",
       "4998  D3369056  http://www.dictionaryofengineering.com/definit...   \n",
       "4999  D2381163  https://en.wikipedia.org/wiki/United_States_mi...   \n",
       "\n",
       "                                                  title  \\\n",
       "0                        Solving Simultaneous Equations   \n",
       "1                                                     .   \n",
       "2                                          nurse's aide   \n",
       "3     Cost Allocation and Cost Apportionment Definit...   \n",
       "4                                                 Equis   \n",
       "...                                                 ...   \n",
       "4995                                                  .   \n",
       "4996           How do you edit and cut off email draft?   \n",
       "4997                                    What Is Rhythm?   \n",
       "4998                                                  .   \n",
       "4999                      United States military ration   \n",
       "\n",
       "                                                    doc  \n",
       "0     Solving Simultaneous Equations Chapter 4 - Alg...  \n",
       "1     Icemelters and Concrete- The basics, what ever...  \n",
       "2     nurse's aide Also found in: Dictionary, Thesau...  \n",
       "3     Home > Encyclopedia > C > Cost Allocation Cost...  \n",
       "4     Wednesday, January 27, 2016 by Mark Robert Ale...  \n",
       "...                                                 ...  \n",
       "4995  Shirazi Meaning & Surname Resources Etymology ...  \n",
       "4996  Min0619 2 Contributions How do you edit and cu...  \n",
       "4997  Learn about Rhythm Home - Fundamentals Sound W...  \n",
       "4998  Nearby Termsvirtual devicevirtual device drive...  \n",
       "4999  From Wikipedia, the free encyclopedianavigatio...  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x10.1 file contains 5000 documents (5000 rows)\n",
    "doc_list = pd.read_csv(\"x10.1\", sep=\"\\t\", header=None, names=[\"doc_id\", \"url\", \"title\", \"doc\"])\n",
    "doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f22ae-6148-4d70-93c2-6c78435ce885",
   "metadata": {},
   "source": [
    "We would like to remove stopwords form our index and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe7c2e1-3c77-433f-bfaf-dc75a9b810b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050d6d0-fc38-49c9-bb1f-37acd6159d74",
   "metadata": {},
   "source": [
    "This class creates a simple inverse document frequency index and provides the ability to search the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afec6b16-52b0-4964-8f15-c4dafa4f9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdfIndex():\n",
    "    \"\"\"Creates a term index based on inverse document frequency (IDF)\n",
    "    \n",
    "    The fomat of self.index is [n, weight, set(doc_ids)]\n",
    "    n is the number of documents that contain the search term.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_list):\n",
    "        self.doc_list = doc_list\n",
    "        self.index = {} # term: [n, weight, set(doc_ids)]\n",
    "        self.tokenizer = BertWordPieceTokenizer(\n",
    "            \"wordpiece/bert-base-uncased-vocab.txt\", lowercase=True)\n",
    "        self.N = 0\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stopwords.add(\"[CLS]\")\n",
    "        self.stopwords.add(\"[SEP]\")\n",
    "        \n",
    "        self.add_docs()\n",
    "        \n",
    "    def add_docs(self):\n",
    "        for row in self.doc_list.itertuples(False):\n",
    "            self.add_doc(row[0], row[3])\n",
    "        self.update_weights()\n",
    "        \n",
    "    def add_doc(self, doc_id, doc):\n",
    "        \"\"\"Adds a single document to the index.\n",
    "        \n",
    "        Since the index stores the number of matching documents,\n",
    "        addition documents can be added to the index at any\n",
    "        time.\n",
    "        \"\"\"\n",
    "        if not isinstance(doc, str):\n",
    "            return\n",
    "        encoding = self.tokenizer.encode(doc)\n",
    "        tokens = set(encoding.tokens)\n",
    "        for tkn in tokens:\n",
    "            self._add_term(tkn, doc_id)\n",
    "        self.N += 1\n",
    "            \n",
    "    def _add_term(self, term, doc_id):\n",
    "        \"\"\"Adds a single term to the index.\n",
    "        \"\"\"\n",
    "        if term in self.stopwords:\n",
    "            return\n",
    "        if term in self.index:\n",
    "            if doc_id not in self.index[term][2]:\n",
    "                self.index[term][0] += 1\n",
    "                self.index[term][1] = None\n",
    "                self.index[term][2].add(doc_id)\n",
    "        else:\n",
    "            self.index[term] = [1, None, set([doc_id])]\n",
    "            \n",
    "    def _calc_weight(self, n):\n",
    "        \"\"\"Calculates an IDF weight for a term using Eq(3).\"\"\"\n",
    "        return math.log((self.N - n + 0.5)/(n + 0.5))\n",
    "    \n",
    "    def update_weights(self):\n",
    "        \"\"\"Updates all weights in the index.\"\"\"\n",
    "        for entry in self.index.values():\n",
    "            entry[1] = self._calc_weight(entry[0])\n",
    "        self.weight_df = pd.DataFrame(\n",
    "            [{\"term\": key, \"count\": val[0], \"weight\": val[1]}\n",
    "             for key, val in self.index.items()])\n",
    "        \n",
    "    def search(self, query):\n",
    "        \"\"\"Accepts a search query and returns matching documents.\"\"\"\n",
    "        # Break query into WordPiece tokens\n",
    "        query_tokens = [tkn for tkn in self.tokenizer.encode(query).tokens\n",
    "                       if tkn not in self.stopwords]\n",
    "        token_doc_pairs = []\n",
    "\n",
    "        # Emit a doc ID, weight, and corresponding token for each\n",
    "        #   document with a term that occurs in the query.\n",
    "        #   (Map phase of mapReduce)\n",
    "        for tkn in query_tokens:\n",
    "            for doc_id in self.index[tkn][2]:\n",
    "                token_doc_pairs.append([doc_id, self.index[tkn][1], [tkn]])\n",
    "                \n",
    "        token_doc_pairs.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Combine weights for same documents\n",
    "        #   (Combine phase)\n",
    "        results = []\n",
    "        current_pair = token_doc_pairs[0]\n",
    "        for next_pair in token_doc_pairs[1:]:\n",
    "            if next_pair[0] == current_pair[0]:\n",
    "                current_pair = [current_pair[0], current_pair[1] + next_pair[1],\n",
    "                               current_pair[2] + next_pair[2]]\n",
    "            else:\n",
    "                results.append(current_pair)\n",
    "                current_pair = next_pair\n",
    "        results.append(current_pair)\n",
    "        \n",
    "        # Put results in a Dataframe\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        for result in results:\n",
    "            result.append(\n",
    "                self.doc_list.loc[self.doc_list.doc_id == result[0],\n",
    "                                  \"title\"].values[0])\n",
    "        results_df = pd.DataFrame(results, columns=[\"Doc ID\",\n",
    "                                                    \"Summed Weights\",\n",
    "                                                    \"Matching Terms\",\n",
    "                                                    \"Title\"])\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b6888-9d93-42fc-ab8a-57c84168036e",
   "metadata": {},
   "source": [
    "Creating the index from the document list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed31f17-3c46-4c5a-97ea-4ba58178997a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error while initializing WordPiece: No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2e47d85b7f3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIdfIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-eca669545538>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, doc_list)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# term: [n, weight, set(doc_ids)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         self.tokenizer = BertWordPieceTokenizer(\n\u001b[0m\u001b[1;32m     11\u001b[0m             \"wordpiece/bert-base-uncased-vocab.txt\", lowercase=True)\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jup/lib/python3.8/site-packages/tokenizers/implementations/bert_wordpiece.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, unk_token, sep_token, cls_token, pad_token, mask_token, clean_text, handle_chinese_chars, strip_accents, lowercase, wordpieces_prefix)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error while initializing WordPiece: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "idf = IdfIndex(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f864f-8ccf-4fd0-980d-63a58cd51840",
   "metadata": {},
   "source": [
    "A simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae3152-df59-4215-b4d2-af73c00f1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.search(\"apple watch tips\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bb1d8-e7fa-4af2-ab13-45a23157b6f1",
   "metadata": {},
   "source": [
    "* The number of times a term appears in a document has no impact on the search results.\n",
    "* A matching term contributes the same weight, no matter what document it appears in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb495cf-5dff-4af1-8952-b84daa5383e6",
   "metadata": {},
   "source": [
    "## 3. Adding Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a0dd7-e9e9-4e4b-9c7a-0c3fe86cef5d",
   "metadata": {},
   "source": [
    "### 1. Math\n",
    "For our next iteration of a weight equation, we'll use:\n",
    "\n",
    "$$w_i(\\textit{tf}) = \\frac{\\textit{tf}}{k + \\textit{tf}} \\cdot w^{IDF}$$\n",
    "\n",
    "* $\\textit{tf}$ is the number of occurrences of term $t$ in document $d$. Note that $\\textit{tf}$ is *not* normalized by dividing by the number of words in a document. $\\textit{tf}$ is a positive integer.\n",
    "* $k$ is a chosen parameter. Robertson and Zaragoza report that values between 1.2 and 2.0 work well.\n",
    "* As $\\textit{tf}$ increases, the value $\\frac{\\textit{tf}}{k + \\textit{tf}}$ approaches 1 asymptotically.  Weights are penalized for having only 1 or a few occurrences of a term, but they do not increase without bound as term frequency increases. Robertson and Zaragoza call this *saturation*.\n",
    "\n",
    "In section 2, there was only one weight for a term, regardless of how many documents it appeared in, because the weight did not depend on how many times a term appeared in a document. With term frequency, there is a different weight for every term-document combination>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e0d32-6470-497e-b457-2016085f5fdf",
   "metadata": {},
   "source": [
    "### 2. Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed669d-ad1c-4f17-9174-98ac7206417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfIndex(IdfIndex):\n",
    "    \"\"\"Index class that incorporates term frequency.\"\"\"\n",
    "    \n",
    "    def __init__(self, doc_list, k):\n",
    "        \"\"\"Constructor\n",
    "        Args:\n",
    "            doc_list: Pandas dataframe of doc_id, url, title, and body.\n",
    "            k: parameter used to modify term frequency.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        super().__init__(doc_list)\n",
    "    \n",
    "    def add_doc(self, doc_id, doc):\n",
    "        \"\"\"Adds a single document to the index.\n",
    "        \n",
    "        Since the index stores the number of matching documents,\n",
    "        addition documents can be added to the index at any\n",
    "        time.\n",
    "        \"\"\"\n",
    "        if not isinstance(doc, str):\n",
    "            return\n",
    "        encoding = self.tokenizer.encode(doc)\n",
    "        tkn_counts = {}\n",
    "        for tkn in encoding.tokens:\n",
    "            if tkn in tkn_counts:\n",
    "                tkn_counts[tkn] += 1\n",
    "            else:\n",
    "                tkn_counts[tkn] = 1\n",
    "\n",
    "        for tkn, ct in tkn_counts.items():\n",
    "            self._add_term(tkn, ct, doc_id)\n",
    "        self.N += 1\n",
    "        \n",
    "    def _add_term(self, tkn, ct, doc_id):\n",
    "        \"\"\"Adds a single term to the index.\n",
    "        \n",
    "        index format: {term: [n, set((doc_id, ct))]}\n",
    "        \"\"\"\n",
    "        if tkn in self.stopwords:\n",
    "            return\n",
    "        if tkn in self.index:\n",
    "            self.index[tkn][0] += 1\n",
    "            self.index[tkn][1][doc_id] = [ct, None]\n",
    "        else:\n",
    "            self.index[tkn] = [1, {doc_id: [ct, None]}]\n",
    "            \n",
    "    def update_weights(self):\n",
    "        for tkn in self.index.values():\n",
    "            n = tkn[0]\n",
    "            w_idf = math.log((self.N - n + 0.5)/(n + 0.5))\n",
    "            for doc_id, val in tkn[1].items():\n",
    "                tf = val[0]\n",
    "                w_i = w_idf * tf / (self.k + tf)\n",
    "                val[1] = w_i\n",
    "                \n",
    "    def search(self, query):\n",
    "        \"\"\"Accepts a search query and returns matching documents.\"\"\"\n",
    "        # Break query into WordPiece tokens\n",
    "        query_tokens = [tkn for tkn in self.tokenizer.encode(query).tokens\n",
    "                       if tkn not in self.stopwords]\n",
    "        token_doc_pairs = []\n",
    "\n",
    "        # Emit a doc ID, weight, and corresponding token for each\n",
    "        #   document with a term that occurs in the query.\n",
    "        #   (Map phase of mapReduce)\n",
    "        for tkn in query_tokens:\n",
    "            n = self.index[tkn][0]\n",
    "            for doc_id, val in self.index[tkn][1].items():\n",
    "                token_doc_pairs.append([doc_id, val[1], [tkn]])\n",
    "                \n",
    "        token_doc_pairs.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Combine weights for same documents\n",
    "        #   (Combine phase)\n",
    "        results = []\n",
    "        current_pair = token_doc_pairs[0]\n",
    "        for next_pair in token_doc_pairs[1:]:\n",
    "            if next_pair[0] == current_pair[0]:\n",
    "                current_pair = [current_pair[0], current_pair[1] + next_pair[1],\n",
    "                               current_pair[2] + next_pair[2]]\n",
    "            else:\n",
    "                results.append(current_pair)\n",
    "                current_pair = next_pair\n",
    "        results.append(current_pair)\n",
    "        \n",
    "        # Put results in a Dataframe\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        for result in results:\n",
    "            result.append(\n",
    "                self.doc_list.loc[self.doc_list.doc_id == result[0],\n",
    "                                  \"title\"].values[0])\n",
    "        results_df = pd.DataFrame(results, columns=[\"Doc ID\",\n",
    "                                                    \"Summed Weights\",\n",
    "                                                    \"Matching Terms\",\n",
    "                                                    \"Title\"])\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592bef9-1db7-4414-845c-83fc4008c3c8",
   "metadata": {},
   "source": [
    "Create a term-frequency index and show a few entries. Set k to 1.5.\n",
    "\n",
    "The format of index entries is `{token: [doc_count, {doc_id: [term_count, term_weight]}]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02b316-e33d-4538-9f2e-fb176a3ddce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tfIndex(doc_list.iloc[:100, :], 1.5)\n",
    "list(tf.index.items())[50:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac5085-e87a-4058-8e35-7d10f935e541",
   "metadata": {},
   "source": [
    "Try out a search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d8d17-755c-47f1-a732-a508d23b0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.search(\"apple watch tips\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b089db0-d584-4d79-9204-ed3f285495c0",
   "metadata": {},
   "source": [
    "Adding term frequency helps. The relevant document is not the first item returned, with a much higher summed weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cde59-212e-4752-a1e4-2250afe0e89e",
   "metadata": {},
   "source": [
    "## 4. Accounting for Document Length - Simple Version of BM25\n",
    "### A. Math\n",
    "Term frequency *tf* can be impacted by the length of a document. As explained in [1], if we don't adjust for length, verbose documents that use more words to deliver the same content will be penalized. However documents may be longer due to having a larger scope (covering more topics) instead of greater verbosity. With the BM25 algorithm, a document length normalization parameter is calculated as such:\n",
    "$$ B := \\left( (1 - b) + b \\frac{\\textit{dl}}{\\textit{avedl}}\\right)$$\n",
    "\n",
    "* $b$ is a parameter chosen by the user. Robertson and Zaragoza report that values between 0.5 and 0.8 work well.\n",
    "* $\\textit{dl}$ is the length of a document.\n",
    "* $\\textit{avedl}$ is the average length of a document in the corpus.\n",
    "\n",
    "Next, the term frequencies are adjusted for document length:\n",
    "\n",
    "$$ \\textit{tf}^\\prime = \\frac{\\textit{tf}}{B}$$\n",
    "\n",
    "Finally:\n",
    "$$ w_i^{BM25} = \\frac{\\textit{tf}^\\prime}{k + \\textit{tf}^\\prime} \\cdot w_i^{IDF}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bc1c3-cf6e-4826-9178-6582791f6763",
   "metadata": {},
   "source": [
    "### B. Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4abfc-a9e8-4b41-a198-566c717bc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bm25Index(tfIndex):\n",
    "    def __init__(self, doc_list, k, b):\n",
    "        \"\"\"Constructor\n",
    "        Args:\n",
    "            doc_list: Pandas dataframe of doc_id, url, title, and body.\n",
    "            k: parameter used to modify term frequency.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.b = b\n",
    "        self.avedl = 0\n",
    "        self.doc_lengths = {}\n",
    "        super().__init__(doc_list, k)\n",
    "        \n",
    "    def add_doc(self, doc_id, doc):\n",
    "        \"\"\"Adds a single document to the index.\n",
    "        \n",
    "        Since the index stores the number of matching documents,\n",
    "        addition documents can be added to the index at any\n",
    "        time.\n",
    "        \"\"\"\n",
    "        if not isinstance(doc, str):\n",
    "            return\n",
    "        encoding = self.tokenizer.encode(doc)\n",
    "        tkn_counts = {}\n",
    "        for tkn in encoding.tokens:\n",
    "            if tkn in tkn_counts:\n",
    "                tkn_counts[tkn] += 1\n",
    "            else:\n",
    "                tkn_counts[tkn] = 1\n",
    "\n",
    "        for tkn, ct in tkn_counts.items():\n",
    "            self._add_term(tkn, ct, doc_id)\n",
    "        self.N += 1\n",
    "        \n",
    "        #### New line\n",
    "        # Track document lengths\n",
    "        self.doc_lengths[doc_id] = len(encoding.tokens)\n",
    "        \n",
    "    def update_weights(self):\n",
    "        # Calcualate average document length\n",
    "        self.avedl = sum(self.doc_lengths.values())/self.N  ## New Line\n",
    "        \n",
    "        for tkn in self.index.values():\n",
    "            n = tkn[0]\n",
    "            w_idf = math.log((self.N - n + 0.5)/(n + 0.5))\n",
    "            for doc_id, val in tkn[1].items():\n",
    "                tf = val[0]\n",
    "                dl = self.doc_lengths[doc_id]\n",
    "                B = (1 - self.b) + self.b * dl / self.avedl\n",
    "                tf_prime = tf / B\n",
    "                w_i_BM25 = w_idf * tf_prime / (self.k + tf_prime)\n",
    "                val[1] = w_i_BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccb0e3-af4a-47a3-8e9a-2a341954cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Bm25Index(doc_list.iloc[:5000, :], 1.5, 0.7)\n",
    "list(tf.index.items())[50:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a939af-459a-439e-a86e-5602cc10fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.search(\"apple watch tips\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3dbd73-7646-403e-9a39-da59f2ffdde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.search(\"solving math equations\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fcc7ff-58ba-49e3-b634-fdec2108d40f",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Notes\n",
    "This version of BM25 assumes no relevance data is available. If relevance data is available, Eq(3) is replaced by:\n",
    "\n",
    "Eq(4)\n",
    "$$ w_i^{RSJ} = \\log \\frac{(r_i + 0.5)(N - R - n_i + r_i + 0.5)}{(n_i - r_i + 0.5)(R - r_i + 0.5)}$$\n",
    "\n",
    "* R is the total number of documents that are judged to be relevant to the query.\n",
    "* r_i is the number of relevant documents that contain term $t_i$.\n",
    "* N is the total number of documents in the corpus.\n",
    "* n_i is the number of documents in the corpus that contain term $t_i$.\n",
    "\n",
    "If the number of relevant documents in the corpus is small compared to the total number of documents, then Eq(4) can be approximated with Eq(3).\n",
    "\n",
    "The BM25 algorithm here is a bit simpler than what was originally published. See section 3.5.1 of [1] for a description of the differences.\n",
    "\n",
    "This example will not and is not meant to scale to the entire Msmarco dataset. It's purpose is to illustrate how a probabilistic IR retrieval algorithm works.\n",
    "\n",
    "For generating indices on large corpora, a MapReduce framework like Spark may be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe454a-1d1b-4cb2-b672-ff69f7a0688f",
   "metadata": {},
   "source": [
    "## Citations\n",
    "[1] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance\n",
    "Framework: BM25 and Beyond. [Foundations and Trends in Information Retrieval\n",
    "(2009).](https://drive.google.com/file/d/1ni_fbufB4irOJTIRzunNQFD23MotEpT7/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
