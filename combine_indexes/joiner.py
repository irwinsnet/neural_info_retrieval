"""Classes for creating indexes from two sets of term weights.

This module works with JSONL data files generated by DeepCT or HDCT.
The files have the format:
```
{"id": "D2322065", "contents": "water water water ... grow"}\n
{"id": "D1055090", "contents": "alberta alberta ... origin"}\n
...
```
The files are suitable for indexing with Lucerne. There is one document
per line.

Stacy Irwin, 18 July 2021
"""

import argparse
import json
import os
import os.path
from posixpath import join
import sys

from tqdm import tqdm

sys.path.append('..')
import util.indexer

class SimpleJoiner():
    """Simply appends two indexible documents.

    Keys in join_data.json:
    * idxNonly_ids: N is index number.
    """

    def __init__(self,
                 idx_paths,
                 join_type='sum_term_weights',
                 join_data_file='join_data.json'):
        assert len(idx_paths) == 2
        # Get paths to all json files for both indices
        self.idx_paths = idx_paths
        self.idx_files = [
            [os.path.join(idx_path, fname) for fname in os.listdir(idx_path)
             if fname[-5:] == '.json']
            for idx_path in idx_paths]
        for idx_file_lst in self.idx_files:
            idx_file_lst.sort()

        # Set Join Type
        if join_type.lower() == 'sum_term_weights':
            self.join_doc_data = self.sum_term_weights
        else:
            raise ValueError('Incorrect join type.')            

        # Open join_data file if it exists
        self.data_file_name = join_data_file
        if os.path.isfile(join_data_file):
            with open(join_data_file) as dfile:
                self.join_data = json.load(dfile)
        else:
            self.join_data = {}
            self.join_data['idx0_only_ids'] = self.get_mismatched_docs(0)
            self.join_data['idx1_only_ids'] = self.get_mismatched_docs(1)
            with open(join_data_file, 'wt') as jdfile:
                json.dump(self.join_data, jdfile)

        # Create or open file indexes
        self.file_indexes = self.get_file_indexes()


    def get_mismatched_docs(self, idx):
        self._check_idx(idx)
        other_idx = (idx + 1) % 2
        print('Identifying doc_ids in index',
                '{} that are not in index {}'.format(idx, other_idx))
        ids = self._get_ids(idx)
        ids_other = self._get_ids(other_idx)
        return list(ids.difference(ids_other))

    def get_file_indexes(self):
        return [[util.msutils.DocIndexJson(fpath) for fpath in flist]
                for flist in self.idx_files]

    def _get_ids(self, idx):
        self._check_idx(idx)
        id_set = set()
        for file in self.idx_files[idx]:
            with open(file) as jfile:
                for line in jfile:
                    data = json.loads(line)
                    id_set.add(data['id'])
        return id_set

    def _check_idx(self, idx):
        assert idx in list(range(len(self.idx_paths)))

    def get_doc(self, idx, doc_id):
        self._check_idx(idx)
        for doc_idx in self.file_indexes[idx]:
            doc = doc_idx[doc_id]
            if doc is not None:
                return doc
        return ''

    def join_indexes(self, output_path):
        for ifile in self.idx_files[0]:
            ifilename = os.path.split(ifile)[1]
            with    open(ifile) as idxfile, \
                    open(os.path.join(output_path, ifilename), 'wt') as ofile:
                for line in tqdm(idxfile, ifilename):
                    doc1_data = json.loads(line)
                    doc1_id = doc1_data['id']
                    doc2_txt = self.get_doc(1, doc1_id)
                    try:
                        doc2_data = json.loads(doc2_txt)
                    except json.JSONDecodeError:
                        doc2_data = None
                    combined_data = self.join_doc_data(doc1_data, doc2_data)
                    ofile.write(json.dumps(combined_data) + '\n')

    def add_missing_docs(self, output_path):
        with open(os.path.join(output_path, 'docs99.json'), 'wt') as ofile:
            for doc_id in self.join_data['idx1_only_ids']:
                doc_txt = self.get_doc(1, doc_id)
                try:
                    doc_data = json.loads(doc_txt)
                except json.JSONDecodeError:
                    continue
                processed_doc_data = self.join_doc_data(doc_data)
                ofile.write(json.dumps(processed_doc_data) + '\n')

    def sum_term_weights(self, doc1_data, doc2_data=None):
        if doc2_data is None:
            doc2_data = {'id': doc1_data['id'], 'contents': ''}
        assert doc1_data['id'] == doc2_data['id']
        return {'id': doc1_data['id'],
                'contents': doc1_data['contents'] + ' ' + doc2_data['contents']}

if __name__ == '__main__':
    desc = ('Combines two file indexes.')

    parser = argparse.ArgumentParser(desc)
    parser.add_argument('Index_0_Path',
                        help='Path to folder with first index.')
    parser.add_argument('Index_1_Path',
                        help='Path to folder with second index.')
    parser.add_argument('Output_Path',
                        help='Path to folder with output files.')
    parser.add_argument('--join-type', default='sum_term_weights',
                        help='Method for combining term weights.')
    
    args = parser.parse_args()

    joiner = SimpleJoiner([args.Index_0_Path, args.Index_1_Path],
                          join_type=args.join_type)
    joiner.join_indexes(args.Output_Path)
        

