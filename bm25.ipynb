{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bdec0b-c2bf-4058-b6e5-2247327bf56e",
   "metadata": {},
   "source": [
    "# BM25 Algorithm\n",
    "BM25 is a probabilistic algorithm for information retrieval that is described in [1]. It is summarized here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683f236-a978-4e31-b80c-b667a7fdee0f",
   "metadata": {},
   "source": [
    "## 1. Msmarco Dataset\n",
    "We'll use the [msmarco](https://microsoft.github.io/msmarco/TREC-Deep-Learning-2019) dataset for examples in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8281ff5-c781-4eb9-88ab-9fe8c4c3bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1555982\thttps://answers.yahoo.com/question/index?qid=20071007114826AAwCFvR\tThe hot glowing surfaces of stars emit energy in the form of electromagnetic radiation.?\tScience & Mathematics Physics The hot glowing surfaces of stars emit energy in the form of electromagnetic radiation.? It is a good approximation to assume that the emissivity e is equal to 1 for these surfaces. Find the radius of the star Rigel, the bright blue star in the constellation Orion that radiates energy at a rate of 2.7 x 10^32 W and has a surface temperature of 11,000 K. Assume that the star is spherical. Use σ =... show more Follow 3 answers Answers Relevance Rating Newest Oldest Best Answer: Stefan-Boltzmann law states that the energy flux by radiation is proportional to the forth power of the temperature: q = ε · σ · T^4 The total energy flux at a spherical surface of Radius R is Q = q·π·R² = ε·σ·T^4·π·R² Hence the radius is R = √ ( Q / (ε·σ·T^4·π) ) = √ ( 2.7x10+32 W / (1 · 5.67x10-8W/m²K^4 · (1100K)^4 · π) ) = 3.22x10+13 m Source (s):http://en.wikipedia.org/wiki/Stefan_bolt...schmiso · 1 decade ago0 18 Comment Schmiso, you forgot a 4 in your answer. Your link even says it: L = 4pi (R^2)sigma (T^4). Using L, luminosity, as the energy in this problem, you can find the radius R by doing sqrt (L/ (4pisigma (T^4)). Hope this helps everyone. Caroline · 4 years ago4 1 Comment (Stefan-Boltzmann law) L = 4pi*R^2*sigma*T^4 Solving for R we get: => R = (1/ (2T^2)) * sqrt (L/ (pi*sigma)) Plugging in your values you should get: => R = (1/ (2 (11,000K)^2)) *sqrt ( (2.7*10^32W)/ (pi * (5.67*10^-8 W/m^2K^4))) R = 1.609 * 10^11 m? · 3 years ago0 1 Comment Maybe you would like to learn more about one of these? Want to build a free website? Interested in dating sites? Need a Home Security Safe? How to order contacts online? \n"
     ]
    }
   ],
   "source": [
    "!head ../data/msmarco/msmarco-docs.tsv -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0784b53-8dd7-472c-9bd2-ec882d9c42ff",
   "metadata": {},
   "source": [
    "Each line of the `msmarco-docs.tsv` contains one document, with four fields per document: document ID, URL, title, and body.\n",
    "\n",
    "The `msmarco-docs.tsv` file contains 3.2 million documents in one 22GB file, which is cumbersome to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abdfcdd-0a12-4f0a-98bd-d40fa3cccde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3213835 ../data/msmarco/msmarco-docs.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc ../data/msmarco/docs/msmarco-docs.tsv -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1814f7-0081-4990-a0f4-9e5eeae63035",
   "metadata": {},
   "source": [
    "The Linux *split* command will split the TSV file into 64 documents, each containing 50,000 documents and several hundred MB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0bacde-8aae-43e4-95b7-de28e0527222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 22G\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  24K May 25 00:18 bm25.ipynb\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4.0K May 24 04:12 wordpiece\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 339M May 24 03:46 x00\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 347M May 24 03:46 x01\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 337M May 24 03:46 x02\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 342M May 24 03:46 x03\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 335M May 24 03:46 x04\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 340M May 24 03:46 x05\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 345M May 24 03:46 x06\n"
     ]
    }
   ],
   "source": [
    "#! split -d -l 50000 ../data/msmarco/msmarco-docs.tsv\n",
    "! ls -l -h | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1381770-c681-4bdf-be2f-7dcf62a1587d",
   "metadata": {},
   "source": [
    "## 2. Simple IDF-ish Algorithm\n",
    "### A. Math\n",
    "Eq (1)\n",
    "$$P(\\textrm{rel}|d, q) \\propto_q \\frac{P(\\textrm{rel}|d, q)}{P(\\overline{\\textrm{rel}}|d, q)}$$\n",
    "\n",
    "$P(\\textrm{rel}|d, q)$ is the probability that document $d$ is relevant for query $q$. Eq(1) shows that the probability is proportional to the odds ratio, which is the probability that the document is relevant over the probability that the document is not relevant.\n",
    "\n",
    "Next, a lot of math and approximations happen and we end up with Eq (2). See section 2.4 of [1] for details.\n",
    "\n",
    "Eq(2)\n",
    "$$P(\\textrm{rel}|d, q) \\propto_q \\sum_{\\textbf{q}, tf_i > 0} w_i$$\n",
    "\n",
    "$w_i$ is the weight for term $t_i$, which represents the importance of term $i$ in document $d$. We calculate the score of a document by summing the weights of all terms that appear in both the query $q$ and the document. We can then return the top $m$ documents in descending order of $P(\\textrm{rel}|d, q)$.\n",
    "\n",
    "A simple way of determining $w_i$ is to use Eq(3), which corresponds to equation 3.3 in [1]. This approach is very similar to a classical *idf* approach.\n",
    "\n",
    "Eq(3)\n",
    "$$w_i^{\\textrm{IDF}} = \\log\\frac{N - n_i + 0.5}{n_i + 0.5}$$\n",
    "\n",
    "N is the number of documents in the corpus and $n_i$ is the number of documents in the corpus that contain term $t_i$. For terms that appear in most documents, $n_i$ will be close to $N$ and $w_i^{\\textrm{IDF}}$ will be close to zero. The values of $0.5$ in the numerator and denominator smooth the weights in the event of rare query terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4242b92-0145-4721-9e2d-f6d15ae055d8",
   "metadata": {},
   "source": [
    "There are about 3.2 million documents in the corpus. Working with a single 22GB file is problematic. Splitting it into files with 50,000 docs each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5f073-6341-45ed-811b-b193132f5a31",
   "metadata": {},
   "source": [
    "### B. Information Retrieval Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d305bf-a193-436c-9939-8861b2edf34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a076cc3-ae09-47e6-82ee-e43ee385fd1d",
   "metadata": {},
   "source": [
    "We will index and create a retrieval system for a corpus of 1000 documents selected from an arbitrary text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5f1ff95a-1820-4489-b7ef-d900855cec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D2273143</td>\n",
       "      <td>https://www.allaboutcircuits.com/textbook/refe...</td>\n",
       "      <td>Solving Simultaneous Equations</td>\n",
       "      <td>Solving Simultaneous Equations Chapter 4 - Alg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1029983</td>\n",
       "      <td>http://xynyth.com/resource/icemelter-concrete/...</td>\n",
       "      <td>.</td>\n",
       "      <td>Icemelters and Concrete- The basics, what ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D312314</td>\n",
       "      <td>http://medical-dictionary.thefreedictionary.co...</td>\n",
       "      <td>nurse's aide</td>\n",
       "      <td>nurse's aide Also found in: Dictionary, Thesau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D582292</td>\n",
       "      <td>https://www.business-case-analysis.com/cost-al...</td>\n",
       "      <td>Cost Allocation and Cost Apportionment Definit...</td>\n",
       "      <td>Home &gt; Encyclopedia &gt; C &gt; Cost Allocation Cost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1021364</td>\n",
       "      <td>http://mexicanspanish.com/equis/</td>\n",
       "      <td>Equis</td>\n",
       "      <td>Wednesday, January 27, 2016 by Mark Robert Ale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>D2634529</td>\n",
       "      <td>https://www.quora.com/What-is-the-difference-b...</td>\n",
       "      <td>What is the difference between FedEx Standard ...</td>\n",
       "      <td>Emily Baker, Previous Fed Ex Office Manager An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>D728098</td>\n",
       "      <td>https://answers.yahoo.com/question/index?qid=2...</td>\n",
       "      <td>How many animals are killed per day?</td>\n",
       "      <td>Pets Other - Pets How many animals are killed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>D3381877</td>\n",
       "      <td>http://www.calculator.pro/Body_Mass_Index.html</td>\n",
       "      <td>Body mass index: BMI calculator for women and men</td>\n",
       "      <td>Enter all data for best result Imperial (lb, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>D3224131</td>\n",
       "      <td>https://www.technorms.com/42603/cancel-netflix...</td>\n",
       "      <td>How to Cancel Your Netflix Service Subscription</td>\n",
       "      <td>Home Web How to Cancel Your Netflix Service Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>D1385337</td>\n",
       "      <td>http://www.chop.edu/conditions-diseases/heart-...</td>\n",
       "      <td>What is a heart murmur?</td>\n",
       "      <td>What is a heart murmur? The heart is a muscula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                                url  \\\n",
       "0    D2273143  https://www.allaboutcircuits.com/textbook/refe...   \n",
       "1    D1029983  http://xynyth.com/resource/icemelter-concrete/...   \n",
       "2     D312314  http://medical-dictionary.thefreedictionary.co...   \n",
       "3     D582292  https://www.business-case-analysis.com/cost-al...   \n",
       "4    D1021364                   http://mexicanspanish.com/equis/   \n",
       "..        ...                                                ...   \n",
       "995  D2634529  https://www.quora.com/What-is-the-difference-b...   \n",
       "996   D728098  https://answers.yahoo.com/question/index?qid=2...   \n",
       "997  D3381877     http://www.calculator.pro/Body_Mass_Index.html   \n",
       "998  D3224131  https://www.technorms.com/42603/cancel-netflix...   \n",
       "999  D1385337  http://www.chop.edu/conditions-diseases/heart-...   \n",
       "\n",
       "                                                 title  \\\n",
       "0                       Solving Simultaneous Equations   \n",
       "1                                                    .   \n",
       "2                                         nurse's aide   \n",
       "3    Cost Allocation and Cost Apportionment Definit...   \n",
       "4                                                Equis   \n",
       "..                                                 ...   \n",
       "995  What is the difference between FedEx Standard ...   \n",
       "996               How many animals are killed per day?   \n",
       "997  Body mass index: BMI calculator for women and men   \n",
       "998    How to Cancel Your Netflix Service Subscription   \n",
       "999                            What is a heart murmur?   \n",
       "\n",
       "                                                   doc  \n",
       "0    Solving Simultaneous Equations Chapter 4 - Alg...  \n",
       "1    Icemelters and Concrete- The basics, what ever...  \n",
       "2    nurse's aide Also found in: Dictionary, Thesau...  \n",
       "3    Home > Encyclopedia > C > Cost Allocation Cost...  \n",
       "4    Wednesday, January 27, 2016 by Mark Robert Ale...  \n",
       "..                                                 ...  \n",
       "995  Emily Baker, Previous Fed Ex Office Manager An...  \n",
       "996  Pets Other - Pets How many animals are killed ...  \n",
       "997  Enter all data for best result Imperial (lb, f...  \n",
       "998  Home Web How to Cancel Your Netflix Service Su...  \n",
       "999  What is a heart murmur? The heart is a muscula...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = pd.read_csv(\"x10\", sep=\"\\t\", header=None, names=[\"doc_id\", \"url\", \"title\", \"doc\"], nrows=1000)\n",
    "doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f22ae-6148-4d70-93c2-6c78435ce885",
   "metadata": {},
   "source": [
    "We would like to remove stopwords form our index and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cfe7c2e1-3c77-433f-bfaf-dc75a9b810b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050d6d0-fc38-49c9-bb1f-37acd6159d74",
   "metadata": {},
   "source": [
    "This class creates a simple inverse document frequency index and provides the ability to search the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "afec6b16-52b0-4964-8f15-c4dafa4f9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index():\n",
    "    \"\"\"Creates a term index based on inverse document frequency (IDF)\n",
    "    \n",
    "    The fomat of self.index is [n, weight, set(doc_ids)]\n",
    "    n is the number of documents that contain the search term.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_list):\n",
    "        self.doc_list = doc_list\n",
    "        self.index = {} # term: [n, weight, set(doc_ids)]\n",
    "        self.tokenizer = BertWordPieceTokenizer(\n",
    "            \"wordpiece/bert-base-uncased-vocab.txt\", lowercase=True)\n",
    "        self.N = 0\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stopwords.add(\"[CLS]\")\n",
    "        self.stopwords.add(\"[SEP]\")\n",
    "        \n",
    "        self.add_docs()\n",
    "        \n",
    "    def add_docs(self):\n",
    "        for row in self.doc_list.itertuples(False):\n",
    "            self.add_doc(row[0], row[3])\n",
    "        self.update_weights()\n",
    "        \n",
    "    def add_doc(self, doc_id, doc):\n",
    "        \"\"\"Adds a single document to the index.\n",
    "        \n",
    "        Since the index stores the number of matching documents,\n",
    "        addition documents can be added to the index at any\n",
    "        time.\n",
    "        \"\"\"\n",
    "        if not isinstance(doc, str):\n",
    "            return\n",
    "        encoding = self.tokenizer.encode(doc)\n",
    "        tokens = set(encoding.tokens)\n",
    "        for tkn in tokens:\n",
    "            self._add_term(tkn, doc_id)\n",
    "        self.N += 1\n",
    "            \n",
    "    def _add_term(self, term, doc_id):\n",
    "        \"\"\"Adds a single term to the index.\n",
    "        \"\"\"\n",
    "        if term in self.stopwords:\n",
    "            return\n",
    "        if term in self.index:\n",
    "            if doc_id not in self.index[term][2]:\n",
    "                self.index[term][0] += 1\n",
    "                self.index[term][1] = None\n",
    "                self.index[term][2].add(doc_id)\n",
    "        else:\n",
    "            self.index[term] = [1, None, set([doc_id])]\n",
    "            \n",
    "    def _calc_weight(self, n):\n",
    "        \"\"\"Calculates an IDF weight for a term using Eq(3).\"\"\"\n",
    "        return math.log((self.N - n + 0.5)/(n + 0.5))\n",
    "    \n",
    "    def update_weights(self):\n",
    "        \"\"\"Updates all weights in the index.\"\"\"\n",
    "        for entry in self.index.values():\n",
    "            entry[1] = self._calc_weight(entry[0])\n",
    "        self.weight_df = pd.DataFrame(\n",
    "            [{\"term\": key, \"count\": val[0], \"weight\": val[1]}\n",
    "             for key, val in self.index.items()])\n",
    "        \n",
    "    def search(self, query):\n",
    "        \"\"\"Accepts a search query and returns matching documents.\"\"\"\n",
    "        # Break query into WordPiece tokens\n",
    "        query_tokens = [tkn for tkn in self.tokenizer.encode(query).tokens\n",
    "                       if tkn not in self.stopwords]\n",
    "        token_doc_pairs = []\n",
    "\n",
    "        # Emit a doc ID, weight, and corresponding token for each\n",
    "        #   document with a term that occurs in the query.\n",
    "        #   (Map phase of mapReduce)\n",
    "        for tkn in query_tokens:\n",
    "            for doc_id in self.index[tkn][2]:\n",
    "                token_doc_pairs.append([doc_id, self.index[tkn][1], [tkn]])\n",
    "                \n",
    "        token_doc_pairs.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Combine weights for same documents\n",
    "        #   (Combine phase)\n",
    "        results = []\n",
    "        current_pair = token_doc_pairs[0]\n",
    "        for next_pair in token_doc_pairs[1:]:\n",
    "            if next_pair[0] == current_pair[0]:\n",
    "                current_pair = [current_pair[0], current_pair[1] + next_pair[1],\n",
    "                               current_pair[2] + next_pair[2]]\n",
    "            else:\n",
    "                results.append(current_pair)\n",
    "                current_pair = next_pair\n",
    "        results.append(current_pair)\n",
    "        \n",
    "        # Put results in a Dataframe\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        for result in results:\n",
    "            result.append(\n",
    "                self.doc_list.loc[self.doc_list.doc_id == result[0],\n",
    "                                  \"title\"].values[0])\n",
    "        results_df = pd.DataFrame(results, columns=[\"Doc ID\",\n",
    "                                                    \"Summed Weights\",\n",
    "                                                    \"Matching Terms\",\n",
    "                                                    \"Title\"])\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b6888-9d93-42fc-ab8a-57c84168036e",
   "metadata": {},
   "source": [
    "Creating the index from the document list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "aed31f17-3c46-4c5a-97ea-4ba58178997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = Index(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f864f-8ccf-4fd0-980d-63a58cd51840",
   "metadata": {},
   "source": [
    "A simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f0ae3152-df59-4215-b4d2-af73c00f1f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc ID</th>\n",
       "      <th>Summed Weights</th>\n",
       "      <th>Matching Terms</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1530634</td>\n",
       "      <td>8.501399</td>\n",
       "      <td>[apple, watch, tips]</td>\n",
       "      <td>Symptoms of Early Menopause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D3010006</td>\n",
       "      <td>8.501399</td>\n",
       "      <td>[apple, watch, tips]</td>\n",
       "      <td>41 Apple Watch tips and tricks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1520949</td>\n",
       "      <td>6.013773</td>\n",
       "      <td>[apple, tips]</td>\n",
       "      <td>Explore Coal Miners, West Virginia, and more!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1557363</td>\n",
       "      <td>6.013773</td>\n",
       "      <td>[apple, tips]</td>\n",
       "      <td>Cecropia Moth - Hyalophora cecropia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D2060971</td>\n",
       "      <td>6.013773</td>\n",
       "      <td>[apple, tips]</td>\n",
       "      <td>Mango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D241774</td>\n",
       "      <td>6.013773</td>\n",
       "      <td>[apple, tips]</td>\n",
       "      <td>The 40 Best Low-Calorie Foods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D2534813</td>\n",
       "      <td>6.013773</td>\n",
       "      <td>[apple, tips]</td>\n",
       "      <td>\"2017 Winner of Best Education Podcast in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D79212</td>\n",
       "      <td>6.013773</td>\n",
       "      <td>[apple, tips]</td>\n",
       "      <td>Do Try This At Home: Hacking Ribs â In The Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D1617231</td>\n",
       "      <td>5.846653</td>\n",
       "      <td>[apple, watch]</td>\n",
       "      <td>'It's another one of his outlandish moves': Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D1794341</td>\n",
       "      <td>5.846653</td>\n",
       "      <td>[apple, watch]</td>\n",
       "      <td>Aslaug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Doc ID  Summed Weights        Matching Terms  \\\n",
       "0  D1530634        8.501399  [apple, watch, tips]   \n",
       "1  D3010006        8.501399  [apple, watch, tips]   \n",
       "2  D1520949        6.013773         [apple, tips]   \n",
       "3  D1557363        6.013773         [apple, tips]   \n",
       "4  D2060971        6.013773         [apple, tips]   \n",
       "5   D241774        6.013773         [apple, tips]   \n",
       "6  D2534813        6.013773         [apple, tips]   \n",
       "7    D79212        6.013773         [apple, tips]   \n",
       "8  D1617231        5.846653        [apple, watch]   \n",
       "9  D1794341        5.846653        [apple, watch]   \n",
       "\n",
       "                                               Title  \n",
       "0                        Symptoms of Early Menopause  \n",
       "1                     41 Apple Watch tips and tricks  \n",
       "2      Explore Coal Miners, West Virginia, and more!  \n",
       "3                Cecropia Moth - Hyalophora cecropia  \n",
       "4                                              Mango  \n",
       "5                      The 40 Best Low-Calorie Foods  \n",
       "6  \"2017 Winner of Best Education Podcast in the ...  \n",
       "7  Do Try This At Home: Hacking Ribs â In The Pre...  \n",
       "8  'It's another one of his outlandish moves': Te...  \n",
       "9                                             Aslaug  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.search(\"apple watch tips\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bb1d8-e7fa-4af2-ab13-45a23157b6f1",
   "metadata": {},
   "source": [
    "* The number of times a term appears in a document has no impact on the search results.\n",
    "* A matching term contributes the same weight, no matter what document it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922543f8-b3be-4a02-b8ee-ce12832783c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39fe454a-1d1b-4cb2-b672-ff69f7a0688f",
   "metadata": {},
   "source": [
    "## Citations\n",
    "[1] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance\n",
    "Framework: BM25 and Beyond. [Foundations and Trends in Information Retrieval\n",
    "(2009).](https://drive.google.com/file/d/1ni_fbufB4irOJTIRzunNQFD23MotEpT7/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
