

export ROBERTA_HOME=/home/ubuntu/efs/roberta
source /home/ubuntu/venv_deepct/bin/activate

# set environment for roberta base
export BERT_BASE_DIR=$ROBERTA_HOME/roberta.base/
export TRAIN_DATA_FILE=./data/marco/myalltrain.relevant.docterm_recall
export OUTPUT_DIR=./output/marco/

# fetch bert, unpack, cleanup
cd $ROBERTA_HOME
wget https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz
gunzip -c roberta.base.tar.gz | tar xf -
rm -Rf roberta.base.tar.gz
cp ../../baseline/uncased_L-12_H-768_A-12/vocab.txt $BERT_BASE_DIR

# get ready to run/test
cd $ROBERTA_HOME/DeepCT
mkdir -p $OUTPUT_DIR

python3 run_deepct.py \
  --task_name=marcodoc \
  --do_train=true \
  --do_eval=false \
  --do_predict=false \
  --data_dir=$TRAIN_DATA_FILE \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/model.pt \
  --max_seq_length=128 \
  --train_batch_size=16 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --recall_field=title \
  --output_dir=$OUTPUT_DIR



## this next block is to predict weights (step 2)
#

# set environment variables that we will require
export BASELINE_HOME=/home/ubuntu/efs/baseline
export BERT_BASE_DIR=$BASELINE_HOME/uncased_L-12_H-768_A-12
export INIT_CKPT=./output/marco/model.ckpt-0
export TEST_DATA_FILE=./data/marco/collection.tsv.1
export OUTPUT_DIR=./predictions/marco/collection_pred_1/

mkdir -p $OUTPUT_DIR

python run_deepct.py \
 --task_name=marcotsvdoc \
 --do_train=false \
 --do_eval=false \
 --do_predict=true \
 --data_dir=$TEST_DATA_FILE \
 --vocab_file=$BERT_BASE_DIR/vocab.txt \
 --bert_config_file=$BERT_BASE_DIR/bert_config.json \
 --init_checkpoint=$INIT_CKPT \
 --max_seq_length=128 \
 --train_batch_size=16 \
 --learning_rate=2e-5 \
 --num_train_epochs=3.0 \
 --output_dir=$OUTPUT_DIR

