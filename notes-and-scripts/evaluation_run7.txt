## NEW WEIGHTING SYSTEM: attempt7

# set environment variables that we will require
export MODEL_HOME=/home/ubuntu/efs/trainvol/trespi
export ANSERINI_HOME=/home/ubuntu/efs/anserini
export RUN_HOME=/home/ubuntu/efs/msmarco_leaderboard_attempt7
export QUERIES=/home/ubuntu/efs/full_dataset_term_weights_10Jul/weights/msmarco_doc_dataset.jsonl
export PASSAGE_WEIGHTS=/home/ubuntu/efs/full_dataset_term_weights_10Jul/predictions/test_results.tsv
export COLLECTION_HDCT=$RUN_HOME/collection_hdct.tsv
export COLLECTION_ANSERINI=/home/ubuntu/efs/max_termweights_no_discount_22Jul/joined_idx
export LUCENE_INDEX=$RUN_HOME/lucene_idx_07172021
mkdir -p $COLLECTION_ANSERINI
mkdir -p $RUN_HOME/runs

cd $ANSERINI_HOME
# generate the Lucene index for the collection of documents
sh target/appassembler/bin/IndexCollection -threads 1 -collection JsonCollection \
 -generator DefaultLuceneDocumentGenerator -input $COLLECTION_ANSERINI \
 -index $LUCENE_INDEX -storePositions -storeDocvectors -storeRaw

# search the collection
target/appassembler/bin/SearchCollection -topicreader TsvInt \
 -index $LUCENE_INDEX \
 -topics src/main/resources/topics-and-qrels/topics.msmarco-doc.dev.txt \
 -output $RUN_HOME/runs/run.msmarco-doc.dev.bm25.txt -bm25

##
# this is for the actual submission and leaderboard judgement
sh target/appassembler/bin/SearchMsmarco -hits 100 -threads 1 \
 -index $LUCENE_INDEX \
 -queries src/main/resources/topics-and-qrels/topics.msmarco-doc.dev.txt \
 -output $RUN_HOME/runs/run.msmarco-doc.leaderboard-dev.bm25base.txt -k1 0.9 -b 0.4

# score for the eval leaderboard
python3 tools/scripts/msmarco/msmarco_doc_eval.py \
 --judgments src/main/resources/topics-and-qrels/qrels.msmarco-doc.dev.txt \
 --run $RUN_HOME/runs/run.msmarco-doc.leaderboard-dev.bm25base.txt 

