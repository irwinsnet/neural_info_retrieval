{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-riverside",
   "metadata": {},
   "source": [
    "# SPIT Version 1\n",
    "SPIT stands for **SP**arse **I**ndex Expansion with **T**ransformers\n",
    "\n",
    "This notebook takes a DistilBert model from the Huggingface Transformers library and adds a classification layer to the output of the CLS token.\n",
    "* DeepCT uses mean squared error as a loss function. None of the out-of-the-box Huggingface models use this loss function, so we'll have to add out own layer to the base model.\n",
    "* Distilbert is a slimmed down version of BERT that requires less memory.\n",
    "* We're using Distilbert while we're debugging the training pipeline because it reqires fewer resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innocent-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-straight",
   "metadata": {},
   "source": [
    "## 1. Import base Distilbert model from Huggingface Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "collectible-cameroon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "=================================================================\n",
      "Total params: 66,362,880\n",
      "Trainable params: 0\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For train-test splits and initialization of weights.\n",
    "random_state=210\n",
    "\n",
    "# Import model and tokenizer\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "config = transformers.DistilBertConfig(output_hidden_states=True)\n",
    "model = transformers.TFDistilBertModel.from_pretrained(model_name,\n",
    "                                                       config=config)\n",
    "\n",
    "# Freeze the pre-trained Distilbert weights, at least for now.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-storm",
   "metadata": {},
   "source": [
    "## 2. Get Some Data\n",
    "The `myalltrain.relevant.docterm_recall` file contains training data from the DeepCT repository. The data consists of passages (`['doc']['title']`) and terms (`['term_recall']` that are known to be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smart-triple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\": \"what kind of animals are in grasslands\", \"term_recall\": {\"grassland\": 1, \"animals\": 1}, \"doc\": {\"position\": \"1\", \"id\": \"4083643\", \"title\": \"Tropical grassland animals (which do not all occur in the same area) include giraffes, zebras, buffaloes, kangaroos, mice, moles, gophers, ground squirrels, snakes, worms, termites, beetles, lions, leopards, hyenas, and elephants.\"}}\n",
      "\n",
      "{\"query\": \"what cause shin splints\", \"term_recall\": {\"caused\": 1, \"splints\": 1, \"shin\": 1}, \"doc\": {\"position\": \"1\", \"id\": \"1773684\", \"title\": \"Shin splints, also known as medial tibial stress syndrome (MTSS), is defined by the American Academy of Orthopaedic Surgeons as pain along the inner edge of the shinbone (tibia).. Shin splints are usually caused by repeated trauma to the connective muscle tissue surrounding the tibia.\"}}\n",
      "\n",
      "{\"query\": \"what type of deficiency is caused by inadequate absorption of a nutrient?\", \"term_recall\": {\"absorption\": 1, \"nutrients\": 1, \"deficiency\": 1, \"caused\": 1, \"inadequate\": 1, \"causes\": 1}, \"doc\": {\"position\": \"1\", \"id\": \"5210357\", \"title\": \"What Causes Iron Deficiency Anemia in People ... often caused by inadequate absorption of vitamin B12 and ... ... of food or interferes directly with the absorption of nutrients. ... inadequate absorption of fats in the digestive ...\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example lines from file\n",
    "data_path = 'myalltrain.relevant.docterm_recall'\n",
    "with open(data_path, 'rt') as dfile:\n",
    "    for _ in range(3):\n",
    "        print(dfile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "analyzed-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_file(path, max_rows=None):\n",
    "    \"\"\"Extracts terms and passages from data file.\"\"\"\n",
    "    data = []\n",
    "    with open(path, 'rt') as dfile:\n",
    "        row_num = 0\n",
    "        for line_txt in dfile:\n",
    "            line_data = json.loads(line_txt)\n",
    "            for term in line_data['term_recall']:\n",
    "                data.append((term, line_data['doc']['title']))\n",
    "            \n",
    "            row_num += 1\n",
    "            if max_rows is not None and row_num >= max_rows:\n",
    "                break\n",
    "    print('Rows read:', row_num)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "traditional-mortality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows read: 1000\n"
     ]
    }
   ],
   "source": [
    "# Read top 1000 rows from data file.\n",
    "data = read_data_file(data_path, max_rows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-catalyst",
   "metadata": {},
   "source": [
    "## 3. Split into Train, Dev, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amino-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training records: 1681\n",
      "Number test/dev records: 1122\n"
     ]
    }
   ],
   "source": [
    "train_data, testdev_data = sklearn.model_selection.train_test_split(\n",
    "    data,\n",
    "    test_size=0.4,\n",
    "    random_state = random_state)\n",
    "print('Number training records:', len(train_data))\n",
    "print('Number test/dev records:', len(testdev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prostate-copper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number dev records: 561\n",
      "Number test records: 561\n"
     ]
    }
   ],
   "source": [
    "dev_data, test_data = sklearn.model_selection.train_test_split(\n",
    "    testdev_data,\n",
    "    test_size=0.5,\n",
    "    random_state = random_state)\n",
    "\n",
    "print('Number dev records:', len(dev_data))\n",
    "print('Number test records:', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-mouse",
   "metadata": {},
   "source": [
    "## 4. Create Labels\n",
    "Due to the source of this data, all candidate terms are relevant. We'll need to figure out how to add irrelevant terms for actual testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "corresponding-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.ones(len(train_data))\n",
    "dev_labels = np.ones(len(dev_data))\n",
    "test_labels = np.ones(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-gauge",
   "metadata": {},
   "source": [
    "## 5. Convert Text to WordPiece Encodings and Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "becoming-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_data, truncation='only_second', padding='max_length')\n",
    "dev_encodings = tokenizer(dev_data, truncation='only_second', padding='max_length')\n",
    "test_encodings = tokenizer(test_data, truncation='only_second', padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reliable-identifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [101, 8116, 102, 2190, 3437, 1024, 1037, 3528, 1997, 2111]\n",
      "Tokens: ['[CLS]', 'deliver', '[SEP]', 'best', 'answer', ':', 'a', 'variety', 'of', 'people']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ids:', train_encodings[0].ids[:10])\n",
    "print('Tokens:', train_encodings[0].tokens[:10])\n",
    "train_encodings[0].type_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "valid-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "narrow-designer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out format of encodings object.\n",
    "e0 = train_encodings[0]\n",
    "e0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-secondary",
   "metadata": {},
   "source": [
    "## 6. Test Base Distilbert Model\n",
    "Obviously we've done no fine tuning. But it's helpful to verify that Distilbert accepts our data as input and produces an output of the appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "center-browse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fbf3819be50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fbf3819be50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(train_dataset.take(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-russian",
   "metadata": {},
   "source": [
    "The output shape is as expected -- a secquence of 512 vectors, each vector having 768 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "generic-dakota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-commercial",
   "metadata": {},
   "source": [
    "## 7. Add Classification Layer to End of Distilbert Model\n",
    "\n",
    "First, we must re-specify the shape of our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aggregate-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(max_len,),\n",
    "                                       name='input_ids',\n",
    "                                       dtype='int32')\n",
    "\n",
    "input_attention_layer = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                                  name='input_attention', \n",
    "                                                  dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-luxembourg",
   "metadata": {},
   "source": [
    "Next we extract the hidden state for only the CLS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "saving-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "cls_token = model(input_ids=input_ids_layer, attention_mask=input_attention_layer).last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-slope",
   "metadata": {},
   "source": [
    "The class token output checks out -- it's a single vector of 768 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "exclusive-singing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf.__operators__.getitem_6')>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-command",
   "metadata": {},
   "source": [
    "Next, create a dense layer with a single output.\n",
    "* **TODO**: Check what activation, if any, is used by DeepCT. It's probably not sigmoid.\n",
    "* **TODO**: Look into other kernal initialization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "center-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_initializer = tf.keras.initializers.GlorotNormal(seed=random_state) \n",
    "output = tf.keras.layers.Dense(1, \n",
    "                               activation='sigmoid',\n",
    "                               kernel_initializer=weight_initializer,  \n",
    "                               kernel_constraint=None,\n",
    "                               bias_initializer='zeros'\n",
    "                               )(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-coaching",
   "metadata": {},
   "source": [
    "Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "wooden-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "spit = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
    "LEARNING_RATE = 5e-5\n",
    "spit.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "successful-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_attention (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
      "                                                                 input_attention[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (Sli (None, 768)          0           tf_distil_bert_model[9][7]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            769         tf.__operators__.getitem_6[0][0] \n",
      "==================================================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 769\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "spit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-instruction",
   "metadata": {},
   "source": [
    "## 8. Test Modified Model\n",
    "\n",
    "Adding the additional layer changed how the model accepts input. We must now pass in the token IDs and attention mask as a list of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "compliant-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_tensor(lst):\n",
    "    return tf.expand_dims(tf.convert_to_tensor(lst, dtype='int32'), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "floating-tackle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.61314005]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds2 = spit.predict([convert_list_to_tensor(train_encodings[0].ids),\n",
    "                      convert_list_to_tensor(train_encodings[0].attention_mask)], 1)\n",
    "preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_spit)",
   "language": "python",
   "name": "conda_spit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
